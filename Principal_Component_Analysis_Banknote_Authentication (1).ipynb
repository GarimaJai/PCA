{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjMXREnZMOiB"
   },
   "source": [
    "# Bank Note Authentication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJq8YCNNMOiC"
   },
   "source": [
    "## Abstract:\n",
    "    \n",
    "Data were extracted from images that were taken for the evaluation of an authentication procedure for banknotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PHmFXZbMOiD"
   },
   "source": [
    "## Data Set Information:\n",
    "    \n",
    "Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an \n",
    "industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and\n",
    "distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool\n",
    "were used to extract features from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLj-HY5uMOiE"
   },
   "source": [
    "Source: https://archive.ics.uci.edu/ml/datasets/banknote+authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkdFmjufMOiF"
   },
   "source": [
    "## Attribute Information:\n",
    "    \n",
    "variance - variance of Wavelet Transformed image (continuous)\n",
    "skewness - skewness of Wavelet Transformed image (continuous)\n",
    "curtosis - curtosis of Wavelet Transformed image (continuous)\n",
    "entropy - entropy of image (continuous)\n",
    "class - class (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCiRr1p3A98c"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yQvsr_hMOiG"
   },
   "outputs": [],
   "source": [
    "#let us start by importing the relevant libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5z6QhxK1MOiJ"
   },
   "outputs": [],
   "source": [
    "#reading the banknote dataset in a dataframe. \n",
    "columns = [\"var\",\"skewness\",\"curtosis\",\"entropy\",\"class\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00267/\\\n",
    "data_banknote_authentication.txt\",index_col=False, names = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SrMlZPaA98g"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgOsJnonMOiU"
   },
   "outputs": [],
   "source": [
    "# we separate the target variable (class) and save it in the y variable. Also the X contains the independant variables.\n",
    "X = df.iloc[:,0:4].values\n",
    "y = df.iloc[:,4].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIUlgdEgMOiW"
   },
   "outputs": [],
   "source": [
    "#splitting the data in test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quD-zlC0MOiY"
   },
   "outputs": [],
   "source": [
    "# scaling the data using the standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_sd = StandardScaler().fit_transform(X_train)\n",
    "X_test_sd = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXpDUmVJMOib"
   },
   "outputs": [],
   "source": [
    "# generating the covariance matrix and the eigen values for the PCA analysis\n",
    "cov_matrix = np.cov(X_train_sd.T) # the relevanat covariance matrix\n",
    "print('Covariance Matrix \\n%s', cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prWl2WUqA98k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generating the eigen values and the eigen vectors, backened working of PCA\n",
    "e_vals, e_vecs = np.linalg.eig(cov_matrix)\n",
    "print('Eigenvectors \\n%s' %e_vecs)\n",
    "print('\\nEigenvalues \\n%s' %e_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEhQNa8bA98l"
   },
   "outputs": [],
   "source": [
    "# ACTUAL PCA ALGOL\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_train_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "263-SoztA98l"
   },
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVlgWGOgA98m"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zezQ1CHKA98m"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWas3kq_A98m"
   },
   "outputs": [],
   "source": [
    "X_train_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B3_RF3vA98n"
   },
   "outputs": [],
   "source": [
    "X_train_sd_pca = pca.transform(X_train_sd)\n",
    "X_train_sd_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXzgOKpMA98n"
   },
   "outputs": [],
   "source": [
    "(-0.24410388*X_train_sd[:,0])+(-0.63914113*X_train_sd[:,1])+(0.61378454*X_train_sd[:,2])+(0.3939295*X_train_sd[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayqmdCxQMOif"
   },
   "outputs": [],
   "source": [
    "# the \"cumulative variance explained\" analysis \n",
    "tot = sum(e_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(e_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjXbgWjwMOih"
   },
   "outputs": [],
   "source": [
    "# Plotting the variance expalained by the principal components and the cumulative variance explained.\n",
    "plt.figure(figsize=(10 , 5))\n",
    "plt.bar(range(1, e_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, e_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nReseBuOMOik"
   },
   "outputs": [],
   "source": [
    "eigen_pairs = [(np.abs(e_vals[i]), e_vecs[:,i]) for i in range(len(e_vals))]\n",
    "eigen_pairs.sort(reverse=True)\n",
    "eigen_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBGrcbuJA98o"
   },
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1].reshape(4,1), \n",
    "                      eigen_pairs[1][1].reshape(4,1)))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzaj-C9MMOin"
   },
   "outputs": [],
   "source": [
    "# generating dimensionally reduced datasets\n",
    "w = np.hstack((eigen_pairs[0][1].reshape(4,1), \n",
    "                      eigen_pairs[1][1].reshape(4,1)))\n",
    "print('Matrix W:\\n', w)\n",
    "X_sd_pca = X_train_sd.dot(w)\n",
    "X_test_sd_pca = X_test_sd.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4CtGOwJMOip"
   },
   "outputs": [],
   "source": [
    "X_train_sd.shape, w.shape, X_sd_pca.shape, X_test_sd_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOO2juCiOT5Y"
   },
   "source": [
    "### We will use Logistic regression, RandomForest and AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQoOPAuGMOi2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_sd, y_train)\n",
    "print ('Before PCA score', model.score(X_test_sd, y_test))\n",
    "\n",
    "model.fit(X_sd_pca, y_train)\n",
    "print ('After PCA score', model.score(X_test_sd_pca, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6b8l0CIA98p"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_sd, y_train)\n",
    "print ('Before PCA score', clf.score(X_test_sd, y_test))\n",
    "\n",
    "clf.fit(X_sd_pca, y_train)\n",
    "print ('After PCA score', clf.score(X_test_sd_pca, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTl-E4HlMOjD"
   },
   "outputs": [],
   "source": [
    "### In the given dataset we trained models with the orginal and dimensionally reduced datsets. The effects of PCA can be clearly appreciated on a fairly large datsaset. The learners are encouraged to try the above with various large datsets out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSosIhH3A98q"
   },
   "outputs": [],
   "source": [
    "## Reference Link \n",
    "* https://www.analyticsvidhya.com/blog/2020/12/an-end-to-end-comprehensive-guide-for-pca/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Principal Component Analysis - Banknote Authentication.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
